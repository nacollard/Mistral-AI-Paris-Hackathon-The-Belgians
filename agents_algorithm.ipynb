{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 528,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "True"
                  ]
               },
               "execution_count": 528,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "from mistralai.client import MistralClient\n",
            "from mistralai.models.chat_completion import ChatMessage\n",
            "from dotenv import load_dotenv\n",
            "from docx import Document\n",
            "from OpenRAG.src.openrag.chunk_vectorization.chunk_vectorization import get_vectorizer\n",
            "from OpenRAG.src.openrag.vectordb.milvus_adapter import init_milvus_connection\n",
            "from pymilvus import Collection\n",
            "import os\n",
            "import cohere\n",
            "import json\n",
            "import re\n",
            "import helper_functions as hf\n",
            "\n",
            "load_dotenv()  # take environment variables from .env."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 529,
         "metadata": {},
         "outputs": [],
         "source": [
            "def send_request_to_mistral_ai(model, messages):\n",
            "    \"\"\"\n",
            "    Send a request to the Mistral AI model and return the response.\n",
            "\n",
            "    Args:\n",
            "        model (str): The Mistral AI model to use.\n",
            "        messages (List[ChatMessage]): The messages to send in the request.\n",
            "\n",
            "    Returns:\n",
            "        str: The response from the Mistral AI model.\n",
            "    \"\"\"\n",
            "    api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
            "    if not api_key:\n",
            "        raise ValueError(\"MISTRAL_API_KEY not found in environment variables.\")\n",
            "    client = MistralClient(api_key=api_key)\n",
            "\n",
            "    try:\n",
            "        chat_response = client.chat(\n",
            "            model=model,\n",
            "            messages=messages,\n",
            "        )\n",
            "\n",
            "        response = chat_response.choices[0].message.content\n",
            "        return response\n",
            "    except Exception as e:\n",
            "        print(f\"Error in Mistral AI request: {e}\")\n",
            "        return None\n",
            "    \n",
            "\n",
            "def load_company_knowledge():\n",
            "    \"\"\"\n",
            "    Load company knowledge from company documents.\n",
            "\n",
            "    Returns:\n",
            "        str: The combined text from all company documents.\n",
            "    \"\"\"\n",
            "    business_model = 'Data/Internal/Business Model de StIT.docx'\n",
            "    long_term_strategy = 'Data/Internal/Plan de développement stratégique sur 8 ans pour StIT.docx'\n",
            "    products_and_services = 'Data/Internal/Produits et services de StIT.docx'\n",
            "    company_docs = [business_model, long_term_strategy, products_and_services]\n",
            "    company_knowledge = ''\n",
            "\n",
            "    for doc in company_docs:\n",
            "        docx_document = Document(doc)\n",
            "        paragraphs_text = ' '.join([paragraph.text for paragraph in docx_document.paragraphs])\n",
            "        company_knowledge += paragraphs_text\n",
            "\n",
            "    return company_knowledge\n",
            "\n",
            "def find_chunks(id, path = \"Data/Internal/HR/\"):\n",
            "    \"\"\"\n",
            "    Find the chunk based on the given id.\n",
            "\n",
            "    Args:\n",
            "        id (int): The id of the chunk to find.\n",
            "        path (str, optional): The path to the chunk files. Defaults to \"Data/Internal/HR/\".\n",
            "\n",
            "    Returns:\n",
            "        dict: A dictionary containing the chunk details, or None if the chunk is not found.\n",
            "    \"\"\"\n",
            "    global_indexing = json.load(open(\"global_indexing.json\", \"r\"))\n",
            "    for key, value in global_indexing.items():\n",
            "        start_idx = value[\"start\"]\n",
            "        end_idx = value[\"end\"]\n",
            "        if start_idx <= id <= end_idx:\n",
            "            index_in_file = id - start_idx\n",
            "            data_dict_file = json.load(open(path + key + \"_chunks.json\", \"r\"))\n",
            "            data_dict_file[\"chunk_\" + str(index_in_file)][\"document\"] = key + \".docx\"\n",
            "            data_dict_file[\"chunk_\" + str(index_in_file)][\"fullpath\"] = path + key + \".docx\"\n",
            "            return data_dict_file[\"chunk_\" + str(index_in_file)]\n",
            "    return None"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 530,
         "metadata": {},
         "outputs": [],
         "source": [
            "def create_prompt_analyst_agent(article, company_knowledge):\n",
            "    \"\"\"\n",
            "    Create the prompt for the Mistral AI model.\n",
            "\n",
            "    Args:\n",
            "        article (str): The news article to analyze.\n",
            "        company_knowledge (str): The company knowledge to include in the prompt.\n",
            "\n",
            "    Returns:\n",
            "        str: The prompt for the Mistral AI model.\n",
            "    \"\"\"\n",
            "    prompt = f\"\"\"\n",
            "                You are an experienced business analyst tasked with determining the priority level of news articles based on their relevance to your company, StIT.\n",
            "\n",
            "                Here is some crucial information about the company to consider during your analysis:\n",
            "                <company_knowledge>{company_knowledge}</company_knowledge>\n",
            "\n",
            "                Please thoroughly read and analyze the following news article:\n",
            "\n",
            "                <article>{article}</article>\n",
            "\n",
            "                ...\n",
            "\n",
            "                After completing your analysis, provide your final assessment in the <output> section, using the following format:\n",
            "\n",
            "                <output>\n",
            "                <priority_level>High OR Medium OR Low</priority_level>\n",
            "                <justification>A detailed explanation of your priority rating, including how the article's main points and key details relate to the company's goals, operations, or industry, and the potential implications and impact of the article on the company</justification>\n",
            "                <main_topic>A one-sentence summary highlighting the article's main topic<main_topic>\n",
            "                </output>\n",
            "\n",
            "                Remember, your goal is to help company management quickly identify and prioritize important news, so be sure to consider the key implications and potential impact of the article on the company in your priority rating and justification.\n",
            "            \"\"\"\n",
            "    return prompt\n",
            "\n",
            "def news_analyst_agent(article):\n",
            "    \"\"\"\n",
            "    Analyze the given news article and determine its priority level and main topic.\n",
            "\n",
            "    Args:\n",
            "        article (str): The news article to analyze.\n",
            "\n",
            "    Returns:\n",
            "        tuple: A tuple containing the priority level (str) and the main topic (str) of the article.\n",
            "    \"\"\"\n",
            "    company_knowledge = load_company_knowledge()\n",
            "\n",
            "    prompt = create_prompt_analyst_agent(article, company_knowledge)\n",
            "\n",
            "    model = \"mistral-large-latest\"\n",
            "\n",
            "    messages = [\n",
            "        ChatMessage(role=\"system\", content=prompt),\n",
            "        ChatMessage(role=\"user\", content=\"Content of the article : \" + article)\n",
            "    ]\n",
            "\n",
            "    xml_response = send_request_to_mistral_ai(model, messages)\n",
            "\n",
            "    priority_level = hf.xml_retriever(xml_response, 'priority_level')\n",
            "    justification = hf.xml_retriever(xml_response, 'justification')\n",
            "    main_topic = hf.xml_retriever(xml_response, 'main_topic')\n",
            "    \n",
            "    return priority_level, justification, main_topic\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 531,
         "metadata": {},
         "outputs": [],
         "source": [
            "# news_article1 = 'Data/External/News Articles/ New tax law in France aims to encourage and support the growth of startups and small businesses copy.docx'\n",
            "# news_article2 = 'Data/External/News Articles/Local bakery in Paris wins award for best croissant in the city copy.docx'\n",
            "# news_article3 = 'Data/External/News Articles/New survey finds that the majority of French people prefer to shop online rather than in-store copy.docx'\n",
            "# news_article4 = 'Data/External/News Articles/Global economic recession expected to impact the tech sector copy.docx'\n",
            "# news_article5 = 'Data/External/News Articles/Massive cyberattack exposes the vulnerabilities of businesses and organizations copy.docx'\n",
            "# news_articles = [news_article1, news_article2, news_article3, news_article4, news_article5]\n",
            "# for news_article in news_articles:\n",
            "#     docx_document = Document(news_article)\n",
            "#     paragraphs_text = ' '.join([paragraph.text for paragraph in docx_document.paragraphs])\n",
            "#     print(news_analyst_agent(paragraphs_text))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 532,
         "metadata": {},
         "outputs": [],
         "source": [
            "def news_agent(article):\n",
            "    \"\"\"\n",
            "    Analyze the given news articles and dispatch them to the appropriate agent.\n",
            "\n",
            "    Args:\n",
            "        articles (List[str]): The news articles to analyze.\n",
            "\n",
            "    Returns:\n",
            "        None\n",
            "    \"\"\"\n",
            "    news_article1 = 'Data/External/News Articles/ New tax law in France aims to encourage and support the growth of startups and small businesses copy.docx'\n",
            "    news_article2 = 'Data/External/News Articles/Local bakery in Paris wins award for best croissant in the city copy.docx'\n",
            "    news_article3 = 'Data/External/News Articles/New survey finds that the majority of French people prefer to shop online rather than in-store copy.docx'\n",
            "    news_article5 = 'Data/External/News Articles/Massive cyberattack exposes the vulnerabilities of businesses and organizations copy.docx'\n",
            "\n",
            "    docx_document = Document(news_article5)\n",
            "    paragraphs_text = ' '.join([paragraph.text for paragraph in docx_document.paragraphs])\n",
            "    priority_level, justification, main_topic = news_analyst_agent(paragraphs_text)\n",
            "    \n",
            "    print(\"Priority Level: \", priority_level)\n",
            "    print(\"Justification: \", justification)\n",
            "    print(\"Main Topic: \", main_topic)\n",
            "        \n",
            "    dispatch_agent(main_topic, justification)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 533,
         "metadata": {},
         "outputs": [],
         "source": [
            "def dispatch_agent(main_topic, justification):\n",
            "    \"\"\"\n",
            "    Determine the employee(s) who should be informed about the given topic and justification.\n",
            "\n",
            "    Args:\n",
            "        main_topic (str): The main topic of the news.\n",
            "        justification (str): The justification for the priority level of the news.\n",
            "\n",
            "    Returns:\n",
            "        None\n",
            "    \"\"\"\n",
            "    results = internal_retriever_agent(main_topic)\n",
            "\n",
            "    to_re_rank = []\n",
            "    for result in results:\n",
            "        content_cv = ''\n",
            "        docx_document = Document(result['fullpath'])\n",
            "        paragraphs_text = ' '.join([paragraph.text for paragraph in docx_document.paragraphs])\n",
            "        content_cv += paragraphs_text\n",
            "        # The input string\n",
            "        filename = result['fullpath']\n",
            "\n",
            "        # The regular expression pattern to match the name\n",
            "        pattern = r'/CV\\s*(.+?)\\.docx'\n",
            "\n",
            "        # Search for the pattern in the input string\n",
            "        match = re.search(pattern, filename)\n",
            "        name = match.group(1)\n",
            "        # The name is in between 'CV' and '.docx'\n",
            "        to_re_rank.append(\"Name: \"+name + \" \" + content_cv)\n",
            "\n",
            "    \"\"\"\n",
            "    co = cohere.Client(os.environ[\"COHERE_API_KEY\"])\n",
            "    rerank_prompt = \"Which of our employees should be informed about this matter ? The matter to be informed is about \" + main_topic + justification\n",
            "    \n",
            "    response = co.rerank(\n",
            "                    model=\"rerank-english-v3.0\",\n",
            "                    query=' '.join(rerank_prompt),\n",
            "                    documents=to_re_rank,\n",
            "                    top_n=7,\n",
            "                )\n",
            "    print(\"############\")\n",
            "    print(\"############\")\n",
            "    print(\"############\")\n",
            "    for el in response:\n",
            "        print(el)\n",
            "    \"\"\"\n",
            "    prompt = \"\"\"\n",
            "                You are a senior executive at StIT, and you have been tasked with identifying the employee who should be informed about a specific matter based on their expertise and role within the company.\n",
            "                Please thoroughly read and analyze the following matter:\n",
            "\n",
            "                <matter>\"\"\"+main_topic + justification+\"\"\"</matter>\n",
            "                Now read carefully the CVs of the following employees and rank them in order of relevance to the matter:\n",
            "                <CVs>\"\"\"+str(to_re_rank)+\"\"\"</CVs>\n",
            "                In the <output> section, write down the names of the 5 people who are the most relevant to contact for this matter. Carefully consider how the main points and key details of this matter. Relate to the provided CVs and job titles at StIT to select the relevant employees. \n",
            "                \n",
            "                After completing your analysis, provide your final assessment in the <output> section, using the following format:\n",
            "\n",
            "                <output>\n",
            "                <employee1>NAME_EMPLOYEE1</employee1>\n",
            "                <employee2>NAME_EMPLOYEE2</employee2>  \n",
            "                <employee3>NAME_EMPLOYEE3</employee3>\n",
            "                <employee4>NAME_EMPLOYEE4</employee4>\n",
            "                <employee5>NAME_EMPLOYEE5</employee5>\n",
            "                </output>\n",
            "\n",
            "                If no Employee is relevant, please write \"None\".\n",
            "                Remember, your goal is to help company management quickly identify and prioritize the employees to inform about the matter, so be sure to consider the key implications and potential impact of the matter on the company in your selection of relevant profiles to inform about it.\n",
            "\n",
            "    \"\"\"\n",
            "\n",
            "    messages = [\n",
            "        ChatMessage(role=\"system\", content=prompt),\n",
            "        ChatMessage(role=\"user\", content=\"The matter : \" + main_topic + justification)\n",
            "    ]\n",
            "\n",
            "    model = \"mistral-large-latest\"\n",
            "\n",
            "    print(send_request_to_mistral_ai(model, messages))\n",
            "    \n",
            "    return None"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 534,
         "metadata": {},
         "outputs": [],
         "source": [
            "def internal_retriever_agent(text, filter='HR'):\n",
            "    \"\"\"\n",
            "    Retrieve internal information related to the given text.\n",
            "\n",
            "    Args:\n",
            "        text (str): The text to search for.\n",
            "        filter (str, optional): The filter to apply to the search results. Defaults to 'HR'.\n",
            "\n",
            "    Returns:\n",
            "        list: A list of dictionaries containing the search results.\n",
            "    \"\"\"\n",
            "    vectorizer = get_vectorizer('mistral')\n",
            "    query_vector = vectorizer.vectorize(text)\n",
            "    \n",
            "    init_milvus_connection()\n",
            "    \n",
            "    collection_name = \"mistral_collection\"\n",
            "    collection = Collection(name=collection_name)\n",
            "    \n",
            "    n_neighbors = 20\n",
            "    results = collection.search([query_vector], \"vector\", param={\"metric_type\": \"L2\", \"params\":{}}, limit=n_neighbors, expr=\"source == '\"+filter+\"'\")\n",
            "    \n",
            "    final_indices = []\n",
            "    for result in results[0]:\n",
            "        if result.id not in final_indices and len(final_indices) + 1 <= n_neighbors:\n",
            "            final_indices.append([result.id, result.distance])\n",
            "            prev_index = result.id - 1\n",
            "            if prev_index >= 0 and prev_index not in final_indices and len(final_indices) + 1 <= n_neighbors and filter != 'HR':\n",
            "                final_indices.append([prev_index, result.distance])\n",
            "            next_index = result.id + 1\n",
            "            if next_index not in final_indices and len(final_indices) + 1 <= n_neighbors and filter != 'HR':\n",
            "                final_indices.append([next_index, result.distance])\n",
            "\n",
            "    results = final_indices\n",
            "    \n",
            "    answer_chunks = []\n",
            "    unique_chunks = []\n",
            "    answer_chunks2 = []\n",
            "    for hit in results:\n",
            "        answer_chunk = find_chunks(hit[0])\n",
            "        if answer_chunk['document'] in unique_chunks and filter == 'HR':\n",
            "            continue\n",
            "        unique_chunks.append(answer_chunk['document'])\n",
            "        answer_chunks.append(answer_chunk['text'])\n",
            "        answer_chunks2.append(answer_chunk)\n",
            "        print(find_chunks(hit[0]))\n",
            "        print(find_chunks(hit[0])['fullpath'])\n",
            "    \n",
            "    return answer_chunks2"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 535,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Priority Level:  High\n",
                  "Justification:  This article describes a massive cyberattack that has affected a wide range of businesses and organizations, including those in the finance, health, retail, and education sectors, which are also the primary target segments of StIT. The cyberattack has resulted in the theft, destruction, and encryption of sensitive and confidential data, posing significant risks to the affected companies' reputation, operations, and financial stability. Given that StIT offers software solutions to these sectors and emphasizes the importance of data security, this news article is highly relevant. It underscores the need for robust cybersecurity measures, which is a critical aspect of StIT's value proposition to its clients. The potential implications include an increased demand for secure software solutions and services, as well as the need for StIT to review and fortify its own cybersecurity practices to protect its clients' data and maintain their trust.\n",
                  "Main Topic:  A massive cyberattack has exposed vulnerabilities in businesses and organizations, highlighting the urgent need for stronger cybersecurity measures.\n"
               ]
            },
            {
               "ename": "AttributeError",
               "evalue": "module 'helper_functions' has no attribute 'find_chunks'",
               "output_type": "error",
               "traceback": [
                  "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
                  "Cell \u001b[0;32mIn[535], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mnews_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
                  "Cell \u001b[0;32mIn[532], line 24\u001b[0m, in \u001b[0;36mnews_agent\u001b[0;34m(article)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJustification: \u001b[39m\u001b[38;5;124m\"\u001b[39m, justification)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMain Topic: \u001b[39m\u001b[38;5;124m\"\u001b[39m, main_topic)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mdispatch_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain_topic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjustification\u001b[49m\u001b[43m)\u001b[49m\n",
                  "Cell \u001b[0;32mIn[533], line 12\u001b[0m, in \u001b[0;36mdispatch_agent\u001b[0;34m(main_topic, justification)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdispatch_agent\u001b[39m(main_topic, justification):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Determine the employee(s) who should be informed about the given topic and justification.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m        None\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43minternal_retriever_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain_topic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     to_re_rank \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
                  "Cell \u001b[0;32mIn[534], line 40\u001b[0m, in \u001b[0;36minternal_retriever_agent\u001b[0;34m(text, filter)\u001b[0m\n\u001b[1;32m     38\u001b[0m answer_chunks2 \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hit \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m---> 40\u001b[0m     answer_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mhf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_chunks\u001b[49m(hit[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m answer_chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m unique_chunks \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHR\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
                  "\u001b[0;31mAttributeError\u001b[0m: module 'helper_functions' has no attribute 'find_chunks'"
               ]
            }
         ],
         "source": [
            "if __name__ == \"__main__\":\n",
            "    news_agent(\"test\")"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.9.6"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
